{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b452fbe",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7fc8bc",
   "metadata": {},
   "source": [
    "This notebook focus on the implementation and exploration of metrics as a way to compare bias in toxic comment classification models. The validation metrics follow the work of [Borkan et al., 2019](https://arxiv.org/abs/1903.04561), including\n",
    "\n",
    "**AUC based metrics**:\n",
    "- Subgroup AUC\n",
    "- Background Positive Subgroup Negative (BPSN) AUC\n",
    "- Background Negative Subgroup Positive (BNSP) AUC\n",
    "\n",
    "**Average Equality Gap**:\n",
    "- Positive AEG\n",
    "- Negative AEG\n",
    "\n",
    "an explanation of the metrics are found below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab5753",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec7cd2",
   "metadata": {},
   "source": [
    "### Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ebbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "NEGATIVE_CROSS_AUC = 'bpsn_auc'\n",
    "POSITIVE_CROSS_AUC = 'bnsp_auc'\n",
    "NEGATIVE_AEG = 'negative_aeg'\n",
    "POSITIVE_AEG = 'positive_aeg'\n",
    "\n",
    "SUBSET_SIZE = 'subset_size'\n",
    "SUBGROUP = 'subgroup'\n",
    "\n",
    "METRICS = [\n",
    "    SUBGROUP_AUC, NEGATIVE_CROSS_AUC, POSITIVE_CROSS_AUC, NEGATIVE_AEG,\n",
    "    POSITIVE_AEG\n",
    "]\n",
    "AUCS = [SUBGROUP_AUC, NEGATIVE_CROSS_AUC, POSITIVE_CROSS_AUC]\n",
    "AEGS = [NEGATIVE_AEG, POSITIVE_AEG]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6eff56",
   "metadata": {},
   "source": [
    "## AUC-Based Metrics\n",
    "\n",
    "The following three metrics are based on the Area Under the Receiver Operating Characteristic Curve (ROC-AUC, or AUC) metric. For any classifier, AUC measures the probability that a randomly chosen negative example will receive a lower score than a randomly chosen positive sample. An AUC of 1.0 means that all negative/positive pairs are all correctly ordered with all negative items receiving lower scores than all positive items.\n",
    "\n",
    "A core benefit of AUC is that is **threshold agnostic**. And AUC of 1.0 also means that is possible to select a threshold that perfectly distinguishes from negative and positive examples.\n",
    "\n",
    "Here, we calculate the metrics by dividing the test data by subroup and comparing its metric with the rest of the data, which its called **\"background\"** data.\n",
    "\n",
    "As an example, consider the following hypothetical score distributions\n",
    "\n",
    "<img src=\"auc_metrics.png\" width=\"900\" height=\"450\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60883fdf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dff69bde",
   "metadata": {},
   "source": [
    "### AUC\n",
    "\n",
    "Uses the scikit-learn implementation of the ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(y_true, y_pred) -> float:\n",
    "    \"\"\"Computes the area under the ROC curve (AUC) for the given true and predicted labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        y_true: array-like of shape (n_samples, ) - True binary labels.\n",
    "        y_pred: array-like of shape (n_samples, ) - Target scores.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        auc: float - The AUC score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError as e:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5f862",
   "metadata": {},
   "source": [
    "### Subgroup AUC\n",
    "\n",
    "Calculates the AUC on only the examples from the subgroups. This represents model understanding and separability within the subgroup itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b62030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgroup_auc(df: pd.DataFrame, subgroup: str, label: str, pred_column: str) -> float:\n",
    "    \"\"\" Computes the AUC for a specific subgroup within the dataset.\n",
    "    The dataframe must have the predicted scores and true labels for the subgroup.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_column: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        auc: float - The AUC score for the specified subgroup.\n",
    "    \"\"\"\n",
    "    # Filters the DataFrame o include only specific subgroup examples\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    # Computes the AUC for the subgroup\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[pred_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d2799",
   "metadata": {},
   "source": [
    "### Background Positive Subgroup Negative (BPSN) AUC\n",
    "\n",
    "Calculates AUC on the positive examples from the background and the negative examples from the subgroup. This value would be reduced when scores for negative examples in the subgroup are higher than scores for other positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72597939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d88c7c8e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
