{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b452fbe",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7fc8bc",
   "metadata": {},
   "source": [
    "TODO: Explain what is bias and fairness\n",
    "\n",
    "\n",
    "This notebook focus on the implementation and exploration of metrics as a way to compare bias in toxic comment classification models. The validation metrics follow the work of [Borkan et al., 2019](https://arxiv.org/abs/1903.04561), including\n",
    "\n",
    "**AUC based metrics**:\n",
    "- Subgroup AUC\n",
    "- Background Positive Subgroup Negative (BPSN) AUC\n",
    "- Background Negative Subgroup Positive (BNSP) AUC\n",
    "\n",
    "**Average Equality Gap**:\n",
    "- Positive AEG\n",
    "- Negative AEG\n",
    "\n",
    "an explanation of the metrics are found below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab5753",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f2474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec7cd2",
   "metadata": {},
   "source": [
    "### Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ebbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "NEGATIVE_CROSS_AUC = 'bpsn_auc'\n",
    "POSITIVE_CROSS_AUC = 'bnsp_auc'\n",
    "NEGATIVE_AEG = 'negative_aeg'\n",
    "POSITIVE_AEG = 'positive_aeg'\n",
    "\n",
    "SUBGROUP_SIZE = 'subgroup_size'\n",
    "SUBGROUP = 'subgroup'\n",
    "\n",
    "METRICS = [\n",
    "    SUBGROUP_AUC, NEGATIVE_CROSS_AUC, POSITIVE_CROSS_AUC, NEGATIVE_AEG,\n",
    "    POSITIVE_AEG\n",
    "]\n",
    "AUCS = [SUBGROUP_AUC, NEGATIVE_CROSS_AUC, POSITIVE_CROSS_AUC]\n",
    "AEGS = [NEGATIVE_AEG, POSITIVE_AEG]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e6764",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6eff56",
   "metadata": {},
   "source": [
    "## AUC-Based Metrics\n",
    "\n",
    "These three metrics are based on the Area Under the Receiver Operating Characteristic Curve (ROC-AUC, or AUC) metric. For any classifier, AUC measures the probability that a randomly chosen negative example will receive a lower score than a randomly chosen positive sample. An AUC of 1.0 means that all negative/positive pairs are all correctly ordered with all negative items receiving lower scores than all positive items.\n",
    "\n",
    "A core benefit of AUC is that is **threshold agnostic**. And AUC of 1.0 also means that is possible to select a threshold that perfectly distinguishes from negative and positive examples.\n",
    "\n",
    "Here, we calculate the metrics by dividing the test data by subgroup $D_{g}$ and comparing its metric with the rest of the data $D$, which its called **\"background\"** data.\n",
    "\n",
    "*New terms:*\\\n",
    "Subgroup data $D_g$: Subset of full data containing examples of subgroup $g$ \\\n",
    "Background data $D$: Set of all examples that does not contain the specific subgroup. ($D \\cap D_g = \\emptyset$)\n",
    "\n",
    "As an example, consider the following hypothetical score distributions for the *background data* (top) and *identity subgroup* (bottom), both divided into negative green examples and positive purple examples.\n",
    "\n",
    "<img src=\"../images/large_score_shift_right.png\">\n",
    "\n",
    "We can see clearly that the examples within the identity receive higher scores, both for positive and negative examples. This score shift is one way that unintended bias can manifest in a model. Many types of unintended bias can be uncovered by looking at differences in the score distribution between background data and data from within a sepcific identity. The following three metrics based on AUC can specifically measure variations in the distribution that cause misordering between negative and positive examples.\n",
    "\n",
    "*(Optional reading)*\\\n",
    "*DEFINITION: Let $D^-$ be the negative examples in the backgroundset, $D^+$ be the positive examples in the background set, $D_{g}^-$ be the negative examples in the identity subgroup, and $D_{g}^+$ be the positive examples in the identity subgroup.*\\\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\textrm{Subgroup AUC} = \\textrm{AUC}(D_{g}^- + D_{g}^+), \\\\\n",
    "\\textrm{BPSN AUC} = \\textrm{AUC}(D^+ + D_{g}^-), \\\\\n",
    "\\textrm{BNSP AUC} = \\textrm{AUC}(D^- + D_{g}^+).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "**Why not use the normal ROC-AUC?**\\\n",
    "As we can see in the previous example, both AUC($D_g$) and AUC($D$) are close to 1.0, however AUC($D_g \\cup D$) is not, since the subgroup negative examples intersect the background positive examples. So why not use the AUC of the full data? Simply because the ROC-AUC does not striclty captures the unintended bias in a model, even tough the AUC score in the example is poor, in many other cases might just mean poor model performance in classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff69bde",
   "metadata": {},
   "source": [
    "### AUC\n",
    "\n",
    "Uses the scikit-learn implementation of the ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccf2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(y_true, y_pred) -> float:\n",
    "    \"\"\"Computes the area under the ROC curve (AUC) for the given true and predicted labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        y_true: array-like of shape (n_samples, ) - True binary labels.\n",
    "        y_pred: array-like of shape (n_samples, ) - Target scores.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        auc: float - The AUC score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError as e:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5f862",
   "metadata": {},
   "source": [
    "### Subgroup AUC\n",
    "\n",
    "Calculates the AUC on only the examples from the subgroups. This represents model understanding and separability within the subgroup itself.\n",
    "\n",
    "<img src=\"../images/subgroup_auc.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b62030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgroup_auc(df: pd.DataFrame, subgroup: str, label: str, pred_col: str) -> float:\n",
    "    \"\"\"Computes the AUC for a specific subgroup within the dataset.\n",
    "    The dataframe must have the predicted scores and true labels for the subgroup.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_col: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        auc: float - The AUC score for the specified subgroup.\n",
    "    \"\"\"\n",
    "    # Filters the DataFrame o include only specific subgroup examples\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    # Computes the AUC for the subgroup\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[pred_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d2799",
   "metadata": {},
   "source": [
    "### Background Positive Subgroup Negative (BPSN) AUC\n",
    "\n",
    "Calculates AUC on the positive examples from the background and the negative examples from the subgroup. This value would be reduced when scores for negative examples in the subgroup are higher than scores for other positive examples.\n",
    "\n",
    "<img src=\"../images/bpsn_auc.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72597939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bpsn_auc(df: pd.DataFrame, subgroup: str, label: str, pred_col: str) -> float:\n",
    "    \"\"\"Computes the AUC of the background positive examples and the within-subgroup negative examples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        df: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_col: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        bpsn_auc: float - The AUC score for the background positive examples and subgroup negative examples.\n",
    "    \"\"\"\n",
    "    # Filters the DataFrame to include only the subgroup NEGATIVE examples...\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    # And the background POSITIVE examples\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = pd.concat(subgroup_negative_examples, non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[pred_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19659be",
   "metadata": {},
   "source": [
    "### Background Negative Subgroup Positive (BNSP) AUC\n",
    "\n",
    "Calculates AUC on the negative examples from the background and the positive examples from the subgroup. This value would be reduced when scores for positive examples in the subgroup are lower than scores for other negative examples.\n",
    "\n",
    "<img src=\"../images/bnsp_auc.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f6a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bnsp_auc(df: pd.DataFrame, subgroup: str, label: str, pred_col: str) -> float:\n",
    "\n",
    "    \"\"\"Computes the AUC of the subgroup positive examples and the background negative examples.\n",
    "    \n",
    "    Parameters  \n",
    "    ----------\n",
    "    df: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_col: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        bnsp_auc: float - The AUC score for the background negative examples and subgroup positive examples.\n",
    "    \"\"\"\n",
    "    # Filters the DataFrame to include only the subgroup POSITIVE examples...\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    # And the background NEGATIVE examples\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = pd.concat(subgroup_positive_examples, non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[pred_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144909a",
   "metadata": {},
   "source": [
    "## Average Equality Gap (AEG)\n",
    "\n",
    "These are two addicional threshold agnostic metrics, built from a generalization of the Equality Gap metric.\n",
    "\n",
    "The Equality gap is the difference between the true positive rate of the subgroup $\\textrm{TPR}(D_{g})$, and the background $\\textrm{TPR}(D)$, at a specific threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d3dcc",
   "metadata": {},
   "source": [
    "### Mann-Whitney U metric (auxiliary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_mwu(data1: pd.DataFrame, data2: pd.DataFrame, pred_col: str) -> float:\n",
    "  \"\"\"Calculate number of datapoints with a higher score in data1 than data2.\"\"\"\n",
    "  scores_1 = data1[pred_col]\n",
    "  scores_2 = data2[pred_col]\n",
    "  n1 = len(scores_1)\n",
    "  n2 = len(scores_2)\n",
    "  if n1 == 0 or n2 == 0:\n",
    "    return None\n",
    "  u, _ = stats.mannwhitneyu(scores_1, scores_2, alternative='less')\n",
    "  return u / (n1 * n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfa458",
   "metadata": {},
   "source": [
    "### Negative AEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_negative_aeg(df: pd.DataFrame, subgroup: str, label: str, pred_col: str) -> float:\n",
    "  mwu = normalized_mwu(df[~df[subgroup] & ~df[label]],\n",
    "                       df[df[subgroup] & ~df[label]], pred_col)\n",
    "  if mwu is None:\n",
    "    return None\n",
    "  return 0.5 - mwu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3151fa",
   "metadata": {},
   "source": [
    "### Postive AEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d405c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_positive_aeg(df: pd.DataFrame, subgroup: str, label: str, pred_col: str) -> float:\n",
    "  mwu = normalized_mwu(df[~df[subgroup] & df[label]],\n",
    "                       df[df[subgroup] & df[label]], pred_col)\n",
    "  if mwu is None:\n",
    "    return None\n",
    "  return 0.5 - mwu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e225a14",
   "metadata": {},
   "source": [
    "# Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42204c",
   "metadata": {},
   "source": [
    "### Compute Subgroup Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18db7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias_metrics_for_subgroup_and_model(dataset: pd.DataFrame,\n",
    "                                                subgroup: str,\n",
    "                                                pred_col: str,\n",
    "                                                label_col: str) -> dict:\n",
    "  \"\"\"Computes per-subgroup metrics for one model and subgroup.\"\"\"\n",
    "  record = {\n",
    "      SUBGROUP: subgroup,\n",
    "      SUBGROUP_SIZE: len(dataset[dataset[subgroup]])\n",
    "  }\n",
    "  record[column_name(model, SUBGROUP_AUC)] = compute_subgroup_auc(\n",
    "      dataset, subgroup, label_col, model)\n",
    "  record[column_name(model, NEGATIVE_CROSS_AUC)] = compute_negative_cross_auc(\n",
    "      dataset, subgroup, label_col, model)\n",
    "  record[column_name(model, POSITIVE_CROSS_AUC)] = compute_positive_cross_auc(\n",
    "      dataset, subgroup, label_col, model)\n",
    "  record[column_name(model, NEGATIVE_AEG)] = compute_negative_aeg(\n",
    "      dataset, subgroup, label_col, model)\n",
    "  record[column_name(model, POSITIVE_AEG)] = compute_positive_aeg(\n",
    "      dataset, subgroup, label_col, model)\n",
    "      \n",
    "  return record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe897a7d",
   "metadata": {},
   "source": [
    "### Compute Model Unintended Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d12c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias_metrics_for_model(dataset: pd.DataFrame,\n",
    "                                   subgroups: list[str],\n",
    "                                   pred_col: str,\n",
    "                                   label_col: str) -> pd.DataFrame:\n",
    "  \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "  records = []\n",
    "  for subgroup in subgroups:\n",
    "    subgroup_record = compute_bias_metrics_for_subgroup_and_model(\n",
    "        dataset, subgroup, pred_col, label_col)\n",
    "    pd.concat(records, subgroup_record)\n",
    "    \n",
    "  return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ea9c6",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e57ed",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_counts(df: pd.DataFrame, score_col: str, label_col: str, threshold: float) -> dict:\n",
    "  return {\n",
    "      'tp': len(df[(df[score_col] >= threshold) & df[label_col]]),\n",
    "      'tn': len(df[(df[score_col] < threshold) & ~df[label_col]]),\n",
    "      'fp': len(df[(df[score_col] >= threshold) & ~df[label_col]]),\n",
    "      'fn': len(df[(df[score_col] < threshold) & df[label_col]]),\n",
    "  }\n",
    "\n",
    "  def compute_confusion_rates(df, score_col, label_col, threshold):\n",
    "  \"\"\"Compute confusion rates.\"\"\"\n",
    "  confusion = confusion_matrix_counts(df, score_col, label_col, threshold)\n",
    "  actual_positives = confusion['tp'] + confusion['fn']\n",
    "  actual_negatives = confusion['tn'] + confusion['fp']\n",
    "  # True positive rate, sensitivity, recall.\n",
    "  tpr = confusion['tp'] / actual_positives\n",
    "  # True negative rate, specificity.\n",
    "  tnr = confusion['tn'] / actual_negatives\n",
    "  # False positive rate, fall-out.\n",
    "  fpr = 1 - tnr\n",
    "  # False negative rate, miss rate.\n",
    "  fnr = 1 - tpr\n",
    "  # Precision, positive predictive value.\n",
    "  precision = confusion['tp'] / (confusion['tp'] + confusion['fp'])\n",
    "  return {\n",
    "      'tpr': tpr,\n",
    "      'tnr': tnr,\n",
    "      'fpr': fpr,\n",
    "      'fnr': fnr,\n",
    "      'precision': precision,\n",
    "      'recall': tpr,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f23a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_subgroup_scatterplots(df,\n",
    "                              subgroup_col,\n",
    "                              values_col,\n",
    "                              title='',\n",
    "                              y_lim=(0.8, 1.0),\n",
    "                              figsize=(15, 5),\n",
    "                              point_size=8,\n",
    "                              file_name='plot'):\n",
    "  \"\"\"Displays a series of one-dimensional scatterplots, 1 scatterplot per subgroup.\n",
    "\n",
    "  Args:\n",
    "    df: DataFrame contain subgroup_col and values_col.\n",
    "    subgroup_col: Column containing subgroups.\n",
    "    values_col: Column containing collection of values to plot (each cell\n",
    "      should contain a sequence of values, e.g. the AUCs for multiple models\n",
    "      from the same family).\n",
    "    title: Plot title.\n",
    "    y_lim: Plot bounds for y axis.\n",
    "    figsize: Plot figure size.\n",
    "  \"\"\"\n",
    "  fig = plt.figure(figsize=figsize)\n",
    "  ax = fig.add_subplot(111)\n",
    "  for i, (_, row) in enumerate(df.iterrows()):\n",
    "    # For each subgroup, we plot a 1D scatterplot. The x-value is the position\n",
    "    # of the item in the dataframe. To change the ordering of the subgroups,\n",
    "    # sort the dataframe before passing to this function.\n",
    "    x = [i] * len(row[values_col])\n",
    "    y = row[values_col]\n",
    "    ax.scatter(x, y, s=point_size)\n",
    "  ax.set_xticklabels(df[subgroup_col], rotation=90)\n",
    "  ax.set_xticks(list(range(len(df))))\n",
    "  ax.set_ylim(y_lim)\n",
    "  ax.set_title(title)\n",
    "  fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8f8b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
