{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b452fbe",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020f022",
   "metadata": {},
   "source": [
    "### Definition of Unintended Bias\n",
    "\n",
    "Every machine learning model is designed to express a bias. For example, a model trained to identify toxic comments is intended to be biased such that comments that are toxic get a higher score than those which are not. The model is not intended to discriminate between the gender of the people expressed in a comment - so if the model does so, we call that unintended bias. Fairness, in contrast, refers to the potential negative impact on society, particularly when different individuals are treated differently.\n",
    "\n",
    "This notebook focuses on the implementation and exploration of metrics to compare bias in toxic comment classification models. The validation metrics follow the implementation of [Borkan et al. (2019)](https://arxiv.org/abs/1903.04561) and [Dixon et al. (2018)](https://dl.acm.org/doi/10.1145/3278721.3278729)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab5753",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f2474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec7cd2",
   "metadata": {},
   "source": [
    "### Defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c53ebbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'\n",
    "BNSP_AUC = 'bnsp_auc'\n",
    "NEGATIVE_AEG = 'negative_aeg'\n",
    "POSITIVE_AEG = 'positive_aeg'\n",
    "\n",
    "SUBGROUP_SIZE = 'subgroup_size'\n",
    "SUBGROUP = 'subgroup'\n",
    "\n",
    "METRICS = [\n",
    "    SUBGROUP_AUC, BNSP_AUC, BNSP_AUC, NEGATIVE_AEG,\n",
    "    POSITIVE_AEG\n",
    "]\n",
    "AUCS = [SUBGROUP_AUC, BPSN_AUC, BNSP_AUC]\n",
    "AEGS = [NEGATIVE_AEG, POSITIVE_AEG]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e6764",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "**AUC-Based Metrics**:\n",
    "- *Subgroup AUC*:\n",
    "- *Background Positive Subgroup Negative (BPSN) AUC*:\n",
    "- *Background Negative Subgroup Positive (BNSP) AUC*:\n",
    "\n",
    "**Average Equality Gap**:\n",
    "- *Positive AEG*\n",
    "- *Negative AEG*\n",
    "\n",
    "an explanation of the metrics are found below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6eff56",
   "metadata": {},
   "source": [
    "## AUC-Based Metrics\n",
    "\n",
    "These three metrics are based on the Area Under the Receiver Operating Characteristic Curve (ROC-AUC, or AUC) metric. For any classifier, AUC measures the probability that a randomly chosen negative example will receive a lower score than a randomly chosen positive sample. An AUC of 1.0 means that all negative/positive pairs are all correctly ordered with all negative items receiving lower scores than all positive items.\n",
    "\n",
    "A core benefit of AUC is that it is **threshold agnostic**. And AUC of 1.0 also means that is possible to select a threshold that perfectly distinguishes from negative and positive examples.\n",
    "\n",
    "Here, we calculate the metrics by dividing the test data by subgroup $D_{g}$ and comparing its metric with the rest of the data $D$, which its called **\"background\"** data.\n",
    "\n",
    "*New terms:*\\\n",
    "Subgroup data $D_g$: Subset of full data containing examples of subgroup $g$ \\\n",
    "Background data $D$: Set of all examples that does not contain the specific subgroup. ($D \\cap D_g = \\emptyset$)\n",
    "\n",
    "As an example, consider the following hypothetical score distributions for the *background data* (top) and *identity subgroup* (bottom), both divided into negative green examples and positive purple examples.\n",
    "\n",
    "<img src=\"../images/large_score_shift_right.png\">\n",
    "\n",
    "We can see clearly that the examples within the identity receive higher scores, both for positive and negative examples. This score shift is one way that unintended bias can manifest in a model. Many types of unintended bias can be uncovered by looking at differences in the score distribution between background data and data from within a sepcific identity. The following three metrics based on AUC can specifically measure variations in the distribution that cause misordering between negative and positive examples.\n",
    "\n",
    "**Why not use the normal ROC-AUC?**\\\n",
    "As we can see in the previous example, both $\\textrm{AUC}(D_g)$ and $\\textrm{AUC}(D)$ are close to 1.0, however $\\textrm{AUC}(D_g \\cup D)$ is not, since the subgroup negative examples intersect the background positive examples, isn't this score a reflection of the bias in the model? So why not use the AUC of the full data instead of separating in subgroup and backgroud? Simply because the ROC-AUC does not strictly capture the unintended bias in a model. Even though the AUC score in the example is poor, in many other cases it might just indicate inferior model performance in classification. \n",
    "\n",
    "*(Optional reading)*\\\n",
    "*DEFINITION: Let $D^-$ be the negative examples in the backgroundset, $D^+$ be the positive examples in the background set, $D_{g}^-$ be the negative examples in the identity subgroup, and $D_{g}^+$ be the positive examples in the identity subgroup.*\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\textrm{Subgroup AUC} = \\textrm{AUC}(D_{g}^- + D_{g}^+), \\\\\n",
    "\\textrm{BPSN AUC} = \\textrm{AUC}(D^+ + D_{g}^-), \\\\\n",
    "\\textrm{BNSP AUC} = \\textrm{AUC}(D^- + D_{g}^+).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff69bde",
   "metadata": {},
   "source": [
    "### AUC\n",
    "\n",
    "Uses the scikit-learn implementation of the ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(y_true, y_pred) -> float:\n",
    "    \"\"\"Computes the area under the ROC curve (AUC) for the given true and predicted labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        y_true: array-like of shape (n_samples, ) - True binary labels.\n",
    "        y_pred: array-like of shape (n_samples, ) - Target scores.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        auc: float - The AUC score, representing the probability that a randomly chosen \n",
    "        negative example will receive a lower score than a randomly chosen positive example.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError as e:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d5f862",
   "metadata": {},
   "source": [
    "### Subgroup AUC\n",
    "\n",
    "Calculates the AUC using only examples from the subgroup. This represents model understanding and separability within the subgroup itself.\n",
    "\n",
    "**Interpretation**: How well does is the model performing to distinguish between toxic and non-toxic comments *specifically within a given identity subgroup*?\n",
    "\n",
    "<img src=\"../images/subgroup_auc.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24b62030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgroup_auc(df: pd.DataFrame, subgroup: str, label: str, pred_col: str) -> float:\n",
    "    \"\"\"Computes the AUC for a specific subgroup within the dataset.\n",
    "    The dataframe must have the predicted scores and true labels for the subgroup.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_col: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        auc: float - The AUC score for the specified subgroup.\n",
    "    \"\"\"\n",
    "    # Filters the DataFrame o include only specific subgroup examples\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    # Computes the AUC for the subgroup\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[pred_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d2799",
   "metadata": {},
   "source": [
    "### Background Positive Subgroup Negative (BPSN) AUC\n",
    "\n",
    "Calculates the AUC using positive examples from the background and negative examples from the subgroup. This value would be reduced when scores for negative examples in the subgroup are higher than scores for other positive examples.\n",
    "\n",
    "**Interpretation**: How often does the model incorrectly score non-toxic comments from a *specific subgroup* **higher** than toxic comments from background data, potentially leading to false positives for that subgroup? \n",
    "\n",
    "\n",
    "| Data               | Comment          | Predicted Score |\n",
    "| ------------------ | ---------------- | ----- |\n",
    "| Background Toxic   | I hate you, die! | 0.85  |\n",
    "| Subgroup Non-Toxic | I am gay!        | 0.9   |\n",
    "\n",
    "\n",
    "<img src=\"../images/bpsn_auc.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72597939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bpsn_auc(df: pd.DataFrame, subgroup: str, label: str, pred_col: str) -> float:\n",
    "    \"\"\"Computes the AUC of the background positive examples and the within-subgroup negative examples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        df: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_col: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        bpsn_auc: float - The AUC score for the background positive examples and subgroup negative examples.\n",
    "    \"\"\"\n",
    "    # Filters the DataFrame to include only the subgroup NEGATIVE examples...\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    # And the background POSITIVE examples\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = pd.concat([subgroup_negative_examples, non_subgroup_positive_examples])\n",
    "    return compute_auc(examples[label], examples[pred_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19659be",
   "metadata": {},
   "source": [
    "### Background Negative Subgroup Positive (BNSP) AUC\n",
    "\n",
    "Calculates the AUC using negative examples from the background and positive examples from the subgroup. This value would be reduced when scores for positive examples in the subgroup are lower than scores for other negative examples.\n",
    "\n",
    "**Interpretation**: How often does the model incorrectly score toxic comments from a *specific subgroup* **lower** than non-toxic comments from background data, potentially leading to false negatives for that subgroup? \n",
    "\n",
    "| Data                 | Comment            | Predicted Score |\n",
    "| -------------------- | ------------------ | --------------- |\n",
    "| Background Non-Toxic | What the heck!     | 0.45            |\n",
    "| Subgroup Toxic       | I hate christians! | 0.40            |\n",
    "\n",
    "<img src=\"../images/bnsp_auc.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f6a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bnsp_auc(df: pd.DataFrame, subgroup: str, label: str, pred_col: str) -> float:\n",
    "\n",
    "    \"\"\"Computes the AUC of the subgroup positive examples and the background negative examples.\n",
    "    \n",
    "    Parameters  \n",
    "    ----------\n",
    "    df: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_col: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        bnsp_auc: float - The AUC score for the background negative examples and subgroup positive examples.\n",
    "    \"\"\"\n",
    "    # Filters the DataFrame to include only the subgroup POSITIVE examples...\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    # And the background NEGATIVE examples\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = pd.concat([subgroup_positive_examples, non_subgroup_negative_examples])\n",
    "    return compute_auc(examples[label], examples[pred_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144909a",
   "metadata": {},
   "source": [
    "## Average Equality Gap (AEG)\n",
    "\n",
    "These are two additional threshold-agnostic metrics, built from a generalization of the Equality Gap metric.\n",
    "\n",
    "The Equality gap is the difference between the true positive rate of the subgroup $\\textrm{TPR}(D_{g})$, and the background $\\textrm{TPR}(D)$ at a specific threshold. Consider the following figure, which plots these rates against each other for every possible threshold t, for some hypothetical classification model.\n",
    "\n",
    "<img src=\"../images/aeg.png\" width=400, height=300>\n",
    "\n",
    "Notice how the hypothetical classifier is biased against the subgroup, as $\\textrm{TPR}(D_{g}) \\lt \\textrm{TPR}(D)$ at different levels at different thresholds. **The shaded area captures the average bias across all thresholds for the classifier**.\n",
    "\n",
    "Another way to generalize the Equality Gap metric is from the perspective of the separability of the score distributions, similar to the AUC metrics in the previous section. With the AUC metrics, we measured mis-orderings between positive and negative examples across the subgroup and background, with the goal of few mis-orderings or high separability. **For the AEGs, we compare positive examples from the subgroup with positive examples from the background, with the goal of low separability**.\n",
    "\n",
    "*DEFINITION (POSITIVE AEG): If a point $i$ (with model score $\\^Y_i$) were chosen uniformly at random from the background data $D^+$ and a point $j$ (with model score $\\^Y_j$) were chosen uniformly at random from our subgroup data $D^+_g$, then the average equality gap is:*\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\textrm{Positive AEG} = \\frac{1}{2} - P \\Bigl\\{\\^Y_i \\gt \\^Y_j | Y_i \\in D^+, Y_j \\in D^+_g\\Bigl\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**What does this mean?**\\\n",
    "Given that both data points are positive examples from the two distributions, the probability that either score is higher than the other should be the same i.e. $\\frac{1}{2}$. We basically want the distributions of both positive examples from the subgroup and backgroup to be similar.\n",
    "\n",
    "Let's compare the two images below\\\n",
    "**Left Image**: The negative examples score distribution from both background and subgroup are identical, the positive scores from the subgroup however, are shifted right, so the probability of randomly choosing a positive example from the background that has greater score than randomly choosing a positive example from the sougroup is zero. Therefore the Positive AEG is equal to 0.5 - 0 = 0.5.\n",
    "\n",
    "**Right Image**: Analogous to the first one, now the Negative AEG is also 0.5. As exercise, think when the AEG assumes a negative score.\n",
    "\n",
    "<img src=\"../images/pos_aeg.png\">\n",
    "\n",
    "<img src=\"../images/pos_neg_aeg.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d3dcc",
   "metadata": {},
   "source": [
    "### Mann-Whitney U Metric\n",
    "\n",
    "You don't really need to read into this function, it basically calculates what is said above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb1d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_mwu(data1: pd.DataFrame, data2: pd.DataFrame, pred_col: str) -> float:\n",
    "    \"\"\"Computes the normalized Mann-Whitney U statistic between two datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        data1: pd.DataFrame - The first dataset.\n",
    "        data2: pd.DataFrame - The second dataset.\n",
    "        pred_col: str - The name of the column to compare.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        normalized_mwu: float - The normalized Mann-Whitney U statistic.\n",
    "    \"\"\"\n",
    "    scores_1 = data1[pred_col]\n",
    "    scores_2 = data2[pred_col]\n",
    "\n",
    "    n1 = len(scores_1)\n",
    "    n2 = len(scores_2)\n",
    "\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return np.nan\n",
    "    u, _ = stats.mannwhitneyu(scores_1, scores_2, alternative='less')\n",
    "    \n",
    "    return u / (n1 * n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfa458",
   "metadata": {},
   "source": [
    "### Negative AEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b95d663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_negative_aeg(df: pd.DataFrame, subgroup: str, label: str, pred_col: str) -> float:\n",
    "    \"\"\"Computes the negative average exposure gain (AEG) for a specific subgroup.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_col: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        negative_aeg: float - The negative AEG score for the specified subgroup.\n",
    "    \"\"\"\n",
    "    mwu = normalized_mwu(df[~df[subgroup] & ~df[label]],\n",
    "                         df[df[subgroup] & ~df[label]], pred_col)\n",
    "    if mwu is None:\n",
    "        return np.nan\n",
    "    return 0.5 - mwu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3151fa",
   "metadata": {},
   "source": [
    "### Postive AEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1332a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_positive_aeg(df: pd.DataFrame, subgroup: str, label: str, pred_col: str) -> float:\n",
    "    \"\"\"Computes the positive average exposure gain (AEG) for a specific subgroup.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_col: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        positive_aeg: float - The positive AEG score for the specified subgroup.\n",
    "    \"\"\"\n",
    "    mwu = normalized_mwu(df[~df[subgroup] & df[label]],\n",
    "                         df[df[subgroup] & df[label]], pred_col)\n",
    "    if mwu is None:\n",
    "        return np.nan\n",
    "    return 0.5 - mwu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e6d97",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "The table below outlines simulated data distributions that exhibit commom biases. Make sure to understand how each metric says about the distribution and vice-versa.\\\n",
    "Pay attention to the sign of the AEG; what does it say about the subgroup score distribution shift? \n",
    "\n",
    "<img src=\"../images/score_examples.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e225a14",
   "metadata": {},
   "source": [
    "# Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42204c",
   "metadata": {},
   "source": [
    "### Compute Subgroup Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa160395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias_metrics_for_subgroup_and_model(dataset: pd.DataFrame,\n",
    "                                                subgroup: str,\n",
    "                                                label: str,\n",
    "                                                pred_col: str) -> dict:\n",
    "    \"\"\"Computes bias metrics for a specific subgroup and model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        dataset: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_col: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        metrics_dict: dict - A dictionary containing the computed bias metrics.\n",
    "    \"\"\"\n",
    "    metrics_dict = {\n",
    "        SUBGROUP: subgroup,\n",
    "        SUBGROUP_SIZE: dataset[subgroup].sum(),\n",
    "        SUBGROUP_AUC: compute_subgroup_auc(dataset, subgroup, label, pred_col),\n",
    "        BPSN_AUC: compute_bpsn_auc(dataset, subgroup, label, pred_col),\n",
    "        BNSP_AUC: compute_bnsp_auc(dataset, subgroup, label, pred_col),\n",
    "        NEGATIVE_AEG: compute_negative_aeg(dataset, subgroup, label, pred_col),\n",
    "        POSITIVE_AEG: compute_positive_aeg(dataset, subgroup, label, pred_col)\n",
    "    }\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe897a7d",
   "metadata": {},
   "source": [
    "### Compute Model Unintended Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "718ba959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias_metrics_for_model(dataset: pd.DataFrame,\n",
    "                                   subgroups: list[str],\n",
    "                                   label: str,\n",
    "                                   pred_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Computes bias metrics for a model across all subgroups in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        dataset: pd.DataFrame - The DataFrame containing the data.\n",
    "        subgroup_col: str - The name of the subgroup column to filter on.\n",
    "        label: str - The name of the true label column.\n",
    "        pred_col: str - The name of the predicted scores column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        metrics_df: pd.DataFrame - A DataFrame containing the computed bias metrics for each subgroup.\n",
    "    \"\"\"\n",
    "    metrics_list = [\n",
    "        compute_bias_metrics_for_subgroup_and_model(dataset, subgroup, label, pred_col)\n",
    "        for subgroup in subgroups\n",
    "    ]\n",
    "    return pd.DataFrame(metrics_list).sort_values(by=SUBGROUP_SIZE, ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
