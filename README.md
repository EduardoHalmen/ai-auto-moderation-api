# ToxBlock: Toxicity Detection System

This project focuses on building a deployable toxicity detection system, including robust text classification models, an API, and a user-friendly interface for testing and visualization. The primary datasets used were the **Jigsaw Unintended Bias in Toxicity Classification** dataset and the **Toxic Comment Classification Challenge**

## Project Goals

- **Data Exploration:** Analyze and visualize the distribution and characteristics of toxic comments.
- **Data Processing:** Clean, preprocess, and engineer features from the raw text data.
- **Modeling:** Implement and evaluate machine learning and deep learning models for toxicity classification.
- **Bias Analysis:** Assess unintended bias in model predictions.
- **Evaluation:** Use appropriate metrics to evaluate model performance and fairness.
- **API:** Implement an API that uses the model to evaluate text
- **UI:** Implement a simple UI to demonstrate the usage of the API and visualize the Model's response

## Datasets:
- [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification)
- [Toxic Comment Classification Challenge](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data)

## Project Structure

```
.
├── api/                     # Encapsulates the api
   ├── app/                  # Implementation of the api
   ├── model/                # Storage of the model
   ├── Dockerfile   
   ├── requirements.txt

├── frontend/                # Encapsulates the UI
   ├── app/                  # Implementation of the UI
   ├── Dockerfile   
   ├── requirements.txt



├── toxicity_model/          # Encapsulates the modeling aspect of the project
   ├── data/                 # Raw data from the Unbiased Challenge
      ├── wikipedia_data/    # Raw data from the Toxic Comment Challenge
      ├── merged_data/       # Processed data from both challenges 
   ├── notebooks/            # Jupyter notebooks for exploration, processing, and modeling
   ├── src/                  # Source code for data processing, modeling, and utilities
   ├── results/              # Model outputs, evaluation metrics, and visualizations
   ├── docker-compose.yml   
   ├── requirements.txt
   └── README.md     
```


## Getting Started

### Running the Full System (API + UI) with Docker Compose:

1. In the project root directory, run:

   ```bash
   docker-compose up --build
   ```
2. Access the system in your browser:

   * **API Documentation (Swagger):** [http://localhost:8000/docs](http://localhost:8000/docs)
   * **UI for Testing:** [http://localhost:8501](http://localhost:8501)


### Running Only the UI:

1. Navigate to the `frontend` directory:

   ```bash
   cd frontend
   ```
2. Build the Docker image:

   ```bash
   docker build -t toxicity-ui .
   ```
3. Run the container:

   ```bash
   docker run -p 8501:8501 toxicity-ui
   ```
4. Access the UI in your browser at: [http://localhost:8501](http://localhost:8501)


### Running Only the API:

1. Navigate to the `api` directory:

   ```bash
   cd api
   ```
2. Build the Docker image:

   ```bash
   docker build -t toxicity-api .
   ```
3. Run the container:

   ```bash
   docker run -p 8000:8000 toxicity-api
   ```
4. Access the API documentation in your browser at: [http://localhost:8000/docs](http://localhost:8000/docs)


### Exploring the `toxicity_model` Directory:

1. Download the datasets from Kaggle and place them inside:

   * `toxicity_model/data/`
   * `toxicity_model/data/wikipedia_data/`

2. Inside the `toxicity_model` directory, run the setup script:

   ```bash
   ./setup.sh
   ```

3. Explore notebooks, models, and scripts as needed.

4. To deactivate the environment:

   ```bash
   deactivate
   ```

---

## Future Work

- Continue improving model performance through advanced techniques and hyperparameter tuning

- Utilize bias analysis results to implement targeted mitigation strategies

- Explore lightweight model alternatives for faster inference if needed

- Expand the system to detect additional categories or languages
